1 d. $SPARK_HOME/bin/spark-shell --packages com.databricks:spark-avro_2.11:4.0.0

import com.databricks.spark.avro._


val sqlContext = new org.apache.spark.sql.SQLContext(sc)


val categories= sqlContext.read.avro("hdfs://localhost/user/vagrant/sparkdataset/category/categories.avro")


categories.show()



For Products: 

import com.databricks.spark.avro._


val sqlContext = new org.apache.spark.sql.SQLContext(sc)


val products= sqlContext.read.avro("hdfs://localhost/user/vagrant/sparkdataset/products/products.avro")


products.show()




1 e. import com.databricks.spark.avro._
     val sqlContext = new org.apache.spark.sql.SQLContext(sc)
     val prd = sqlContext.read.avro("hdfs://localhost/user/vagrant/sparkdataset/products/products.avro")

     import org.apache.spark.rdd.RDD
     import org.apache.spark.sql.Row
     val df = prd.selectExpr("_1","_2","_3","cast(_4 as float) _4","_5")
     val rdd_rws: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = df.rdd
     val Nrdd = rdd_rws.filter(t => (t(3) != null))
     
     val srcrdd = Nrdd.map(_.mkString(","))
     val a = srcrdd.map(_.split(",")).filter(_(3).toFloat < 100)
     val msp = a.map(_.mkString(","))

     msp.coalesce(1).saveAsTextFile("hdfs://localhost/user/vagrant/output/4/")
     hadoop fs -cat output/4/part-00000


1f. import com.databricks.spark.avro._
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    val prd= sqlContext.read.avro("hdfs://localhost/user/vagrant/sparkdataset/products/products.avro")
    import org.apache.spark.rdd.RDD
    import org.apache.spark.sql.Row
    val df = prd.selectExpr("_1","_2","_3","cast(_4 as float) _4","_5")
    val rdd_rws: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = df.rdd
    val Nrdd = rdd_rws.filter(t => (t(3) != null))
    val srcrdd = Nrdd.map(_.mkString(","))
    val a = srcrdd.map(_.split(",")).filter(_(3).toFloat < 100)
    val msp = a.map(_.mkString(","))

   import com.databricks.spark.avro._
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   val category= sqlContext.read.avro("hdfs://localhost/user/vagrant/sparkdataset/category/categories.avro")
   case class products(product_id: String, category_id: String, product_name: String, price: String, link: String)
   val text = msp.map(line => line.split(",").map(_.trim))
   val prd = text.map{case Array(s0, s1, s2, s3,s4) => product(s0, s1, s2, s3,s4)}.toDF
   val catgry_seqn = Seq("category_id", "category_name")
   val catgrydf = category.toDF(catgry_seqn: _*)
   val catget = catgrydf.selectExpr("category_id","category_name")
   val joining_df = catgry.join(prd, "category_id")

    import org.apache.spark.sql.expressions.Window
    val bdf =  joining_df.select(col("category_id"), col("category_name"), col("product_name"), col("price") ,row_number().over(Window.partitionBy(col("category_id")).orderBy(col("price").desc)).alias("Row_Num"))
    val filldf = bdf.where(bdf("Row_Num") <=10)
    val selctDF = filldf.select(col("category_name"),col("product_name"),col("price"))
    val sortingDF = selctdf.sort(col("category_name"),col("product_name"),col("price"))
    val rdd_f: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = sortDF.rdd
    val rdd_df = rdd_f.map(_.mkString("\t"))
    rdd_df.coalesce(1).saveAsTextFile("hdfs://localhost/user/vagrant/output/5/")
    hadoop fs -cat output/5/part-00000
     


1 g.    